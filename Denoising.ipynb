{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c17bef5",
   "metadata": {},
   "source": [
    "- Scikit pipeline?\n",
    "- Napravi grader da proceni koliko je tezak primer i primeni na svim\n",
    "- Import podataka -> Train/Valid split N puta -> Generisanje suma samo za train (da bi mogli da se uverimo da zaista daje dobre rezultate, ako poremetimo valid set mozda greskom dobijemo dobre ili lose rez.) -> \n",
    "    Transformacije (SkewTest+Norm/Stand, SMOTE, Method specific transf) - train na train, apply na valid ->\n",
    "    Trening na train, validacija na valid -> viz i cuvanje\n",
    "- Sacuvaj - za svaku kombinaciju transf, modela i generisanje suma po jedan entry u pd da bi uzeli najbolje\n",
    "- Vizuelizacije - Preciznost standardna za svaki difficulty + broj pogodjenih?, za difficulty-e, grafikon za top 5-10 metoda kao linija, grafikon gde su outlieri x-ovi ali zeleni su dobro a crveni lose klasifikovani\n",
    "- Ako izbacujes ispisi koliko lakih, srednjih, teskih je izbaceno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dece0e",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca626a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub openpyxl imbalanced-learn seaborn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5174720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub as kg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_predict\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler, LabelEncoder\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from helper.plot import plot_attr_label, plot_attributes, plot_difficulties\n",
    "from helper.transform import transform_pca, remove_outliers_zscore, remove_outliers_db, remove_outliers_isf, bin_attributes_mean, bin_attributes_median, regression_reduce_noise\n",
    "from helper.transform import remove_label_noise_ensemble_filter, remove_label_noise_cross_validated_committees_filter, remove_label_noise_iterative_partitioning_filter\n",
    "from helper.train import run_models, test_models, difficulty_split\n",
    "from helper.noise import add_attribute_noise\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from typing import Literal, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dad7e8",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kg.dataset_download(\"muratkokludataset/dry-bean-dataset\")\n",
    "file_name = '/Dry_Bean_Dataset/Dry_Bean_Dataset.xlsx'\n",
    "print(\"Downloaded at: \", path)\n",
    "data = pd.read_excel(path+file_name)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_n_split(data: pd.DataFrame, train_percentage: float, encoder: TransformerMixin, class_column:str) -> tuple[pd.DataFrame, pd.DataFrame, str] :\n",
    "    \"\"\"\n",
    "        returns tuple containing train set, test set and string representing the label column and the trained encoder\n",
    "    \"\"\"\n",
    "    datacp = data.copy()\n",
    "    encoded_class_column = f\"Class_{encoder.__class__.__name__}\"\n",
    "    datacp[encoded_class_column] = encoder.fit_transform(datacp[class_column])\n",
    "    datacp.drop(inplace=True, axis=1, labels=[class_column])\n",
    "\n",
    "    test_percentage = 1 - train_percentage\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=test_percentage, random_state=42)\n",
    "\n",
    "    # Perform the split\n",
    "    for train_idx, test_idx in split.split(datacp, datacp[encoded_class_column]):\n",
    "        train_set = datacp.iloc[train_idx]\n",
    "        test_set = datacp.iloc[test_idx]\n",
    "\n",
    "    print('Train size: ', len(train_set), 'x', len(train_set.iloc[0]))\n",
    "    print('Test size: ', len(test_set), 'x', len(test_set.iloc[0]))\n",
    "    print(f'Encoded classes in column \"{encoded_class_column}\"')\n",
    "    return (train_set, test_set, encoder, encoded_class_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, encoder, encoded_class_column = encode_n_split(data, 0.8, LabelEncoder(), 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce52028",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[encoded_class_column]\n",
    "y_test = test_set[encoded_class_column]\n",
    "X_train = train_set.drop(axis = 1, labels=[encoded_class_column])\n",
    "X_test = test_set.drop(axis = 1, labels=[encoded_class_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b253e",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes, class_counts = np.unique(y_train, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.plot import plot_attr_label, plot_attributes, plot_difficulties\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7150081",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attributes(X_train, label_column = encoded_class_column, n_attrs = len(class_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61382db1",
   "metadata": {},
   "source": [
    "Atributi \n",
    "- Area\n",
    "- Perimeter\n",
    "- Major Axis Length\n",
    "- Minor Axis Length\n",
    "- AspectRation\n",
    "- ConvexArea\n",
    "- EquivDiameter \n",
    "\n",
    "imaju dosta velike vrednosti dok ostali atributi su u range $[0-1]$ \\\n",
    "Sa grafikona vidimo da vrednosti atributa ne prate normalnu raspodelu.\\\n",
    "Atributi kao ShapeFactor4 i Solidity imaju velike repove. Zato ćemo tokom preprocesiranja normalizovati raspodele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49966f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.countplot(data = y_train.to_frame(), x = encoded_class_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2936f",
   "metadata": {},
   "source": [
    "Pored toga skup podataka nije balansiran:\n",
    "- Klasa 1 je slabo zastupljena sa manje od 500 instanci, klasa 0 ima oko 1000 dok klasa 3 dominira sa oko 2500 instanci\n",
    "- Zato ćemo izvršiti under i oversampling na oko 500-1000 instanci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bebb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sns.pairplot(pd.concat([X_train, y_train.rename('Class')], axis=1), hue='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c24bb",
   "metadata": {},
   "source": [
    "Po grafikonu iznad vidimo da se klasa 1 karakteriše visokim vrednostima Major i Minor axis length, EquivDiameter, ConvexArea, Area i Perimeter. Pored toga ima male vrednosti ShapeFactor1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da68c59",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#1212AA; height:auto; border-radius:10px; padding:16px; width:600px; color:white\">\n",
    "<h3>Zaključci</h3>\n",
    "<ul>\n",
    "<li>Podaci su nebalansirani, potrebno je under i over sample-ovati na oko 1-1.5 hiljade instanci</li>\n",
    "<li>Atribute je potrebno normalizovati, a neke i skalirati kao što su Area</li>\n",
    "<li>Klasa 1 je nedovoljno zastupljena ali lako prepoznatljiva po atributima koji imaju visoke vrednosti kao što je Area</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43339981",
   "metadata": {},
   "source": [
    "# Normalizacija/Standardizacija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "X_train_transformed = pt.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a52419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samo power transform\n",
    "X_train_transformed = pd.DataFrame(X_train_transformed, columns=X_train.columns)\n",
    "sns.pairplot(pd.concat([X_train_transformed, y_train.rename('Class')], axis=1), hue='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bafce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PowerTransform + MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_set_scaled = scaler.fit_transform(X_train_transformed)\n",
    "X_train_transformed = pd.DataFrame(train_set_scaled, columns=X_train.columns)\n",
    "sns.pairplot(pd.concat([X_train_transformed, y_train.rename('Class')], axis=1), hue='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samo MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_set_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(train_set_scaled, columns=X_train.columns)\n",
    "sns.pairplot(pd.concat([X_train_scaled, y_train.rename('Class')], axis=1), hue='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestClassifiers(\n",
    "    dataPredictorPairs: list[tuple[pd.DataFrame, pd.DataFrame, ClassifierMixin, str]],\n",
    "    n_columns=4,\n",
    "    cv=5,\n",
    "    normalize = \"pred\"\n",
    "):\n",
    "    test_results = []\n",
    "    n_rows = len(dataPredictorPairs) // n_columns\n",
    "    if n_rows == 0:\n",
    "        n_rows = 1\n",
    "        n_columns = len(dataPredictorPairs)\n",
    "    elif n_rows * n_columns < len(dataPredictorPairs):\n",
    "        n_rows += 1\n",
    "\n",
    "    _, axes = plt.subplots(n_rows, n_columns, figsize=(5 * n_columns, 4 * n_rows))\n",
    "    axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "    for i, (X, y, predictor, test_name) in enumerate(dataPredictorPairs):\n",
    "        # Get cross-validated predictions\n",
    "        y_pred = cross_val_predict(predictor, X, y, cv=cv)\n",
    "        accuracy = round(accuracy_score(y, y_pred), 2)\n",
    "        cm = confusion_matrix(y, y_pred, normalize=normalize)\n",
    "        test_results.append((test_name, accuracy, cm))\n",
    "\n",
    "        ConfusionMatrixDisplay(cm).plot(ax=axes[i], colorbar=False)\n",
    "        axes[i].set_title(f\"{test_name}\\nCV Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(len(dataPredictorPairs), len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ace04",
   "metadata": {},
   "source": [
    "Na osnovu prethodnih grafikona mozemo da zakljucimo da `PowerTransform` i `MinMaxScaler` pogorsavaju separaciju razlicitih klasa.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08513ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestClassifiers(\n",
    "    dataPredictorPairs=[\n",
    "        (X_train, y_train, RandomForestClassifier(n_jobs=-1, n_estimators=20), \"Without transforms\"),\n",
    "        (X_train_transformed, y_train, RandomForestClassifier(n_jobs=-1, n_estimators=20), \"With power and scaling transforms\"),\n",
    "        (X_train_scaled, y_train, RandomForestClassifier(n_jobs=-1, n_estimators=20), \"With scaling only\"),\n",
    "    ], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestClassifiers(\n",
    "    dataPredictorPairs=[\n",
    "        (X_train, y_train, SVC(), \"Without transforms\"),\n",
    "        (X_train_transformed, y_train, SVC(), \"With power and scaling transforms\"),\n",
    "        (X_train_scaled, y_train, SVC(), \"With scaling only\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf916f18",
   "metadata": {},
   "source": [
    "Međutim, rezultati pokazuju bolji accuracy kada se primeni Power i Scaling transformacija. Ovo se vidi i po accuracy score-u kao i po konfuzionoj matrici.\\\n",
    "Zato na dalje koristimo transformisani skup podataka.\\\n",
    "Iako ima malo instanci klase 1, klasifikacija ovih instanci je jednostavna za oba modela.\n",
    "Sa druge strane, iako model ima najviše instanci klase 3 ova klasa je najteža za klasifikovati. Ovo ukazuje na overfitting ili šum među instancama klase 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e67b43",
   "metadata": {},
   "source": [
    "# Under/OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21111821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "target_samples = 1700 # Pick a number in the range\n",
    "\n",
    "# Oversample with SMOTE and undersample with RandomUnderSampler\n",
    "smote = SMOTE(sampling_strategy=lambda y: {k: max(target_samples, v) for k, v in Counter(y).items() if v < target_samples}, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy=lambda y: {k: min(target_samples, v) for k, v in Counter(y).items() if v > target_samples}, random_state=42)\n",
    "X_train_resampled, y_train_resampled = under.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.countplot(data = y_train_resampled.to_frame(), x = encoded_class_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ed65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestClassifiers(\n",
    "    dataPredictorPairs=[\n",
    "        (X_train_transformed, y_train, RandomForestClassifier(n_jobs=-1, n_estimators=20), \"RandomForest, No Over/Undersample\"),\n",
    "        (X_train_resampled, y_train_resampled, RandomForestClassifier(n_jobs=-1, n_estimators=20), \"RandomForest + SMOTE + Undersample\"),\n",
    "        (X_train_transformed, y_train, SVC(), \"SVC, No Over/Undersample\"),\n",
    "        (X_train_resampled, y_train_resampled, SVC(), \"SVC + SMOTE + Undersample\"),\n",
    "    ], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1caf4f1",
   "metadata": {},
   "source": [
    "Vršenjem under i oversamplinga dobijamo malo bolje rezultate, pa na dalje koristimo `X_train_resampled`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5673bd",
   "metadata": {},
   "source": [
    "# Difficulty split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difficulty_split(X, y, models, difficulty_categories=3, cv=5):\n",
    "    \"\"\"\n",
    "    Estimate difficulty of each sample based on how many models misclassify it during cross-validation.\n",
    "    \"\"\"\n",
    "    wrong_prediction_count = np.zeros(len(y))\n",
    "\n",
    "    for model in models:\n",
    "        y_pred = cross_val_predict(model, X, y, cv=cv)\n",
    "        wrong_prediction_count += (y_pred != y).astype(int)\n",
    "\n",
    "    # Define bins for difficulty categories (e.g. 0 wrong -> easy, 1-2 -> medium, etc.)\n",
    "    bins = np.linspace(0, len(models), num=difficulty_categories+1)[1:-1]  # exclude first and last bin edges\n",
    "    difficulty_labels = np.digitize(wrong_prediction_count, bins, right=False)\n",
    "\n",
    "    # Map difficulty labels to sample indices\n",
    "    difficulty_indices = {\n",
    "        int(i): np.where(difficulty_labels == i)[0]\n",
    "        for i in np.unique(difficulty_labels)\n",
    "    }\n",
    "\n",
    "    return wrong_prediction_count, difficulty_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bd897",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs = -1)\n",
    "knc = KNeighborsClassifier(n_jobs = -1)\n",
    "gbc = GradientBoostingClassifier()\n",
    "svc = SVC()\n",
    "mlp = MLP(hidden_layer_sizes=[16,16,16], alpha=0)\n",
    "miss_count, indices = difficulty_split(X_train_resampled, y_train_resampled, models=[rfc, knc, gbc, svc, mlp], difficulty_categories=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f157b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "def plot_difficulties_here(X, indices_map):\n",
    "    iso = Isomap(n_components=2, n_jobs=-1)\n",
    "    X_lowered = iso.fit_transform(X)\n",
    "    \n",
    "    color_map = {\n",
    "        1: \"green\",\n",
    "        2: \"orange\",\n",
    "        3: \"red\"\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    for difficulty, idxs in indices_map.items():\n",
    "        color = color_map.get(difficulty, \"blue\")  # Default color if difficulty is missing\n",
    "        sns.scatterplot(x=X_lowered[idxs, 0], y=X_lowered[idxs, 1],\n",
    "                        color=color, alpha=0.5, edgecolor='k', label=f'Difficulty {difficulty}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Isomap Dimension 1\")\n",
    "    plt.ylabel(\"Isomap Dimension 2\")\n",
    "    plt.title(\"Instance Difficulty Visualization\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_difficulties_here(X_train_resampled, indices)\n",
    "for k in indices.keys():\n",
    "    print(f\"{len(indices[k])} instances in difficulty {k}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f73a5",
   "metadata": {},
   "source": [
    "- [x] CV za train+test\n",
    "- [x] Baseline performanse PO DIFFICULTYxMODEL\n",
    "- [ ] Kako se sve dodaje shum?\n",
    "- [ ] Smisli plot type kad se doda i shum\n",
    "- [ ] Metode za uklanjanje suma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae202551",
   "metadata": {},
   "source": [
    "# Baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_pairs(X: pd.DataFrame, y: pd.DataFrame, models: list[ClassifierMixin], difficulty_indices:dict[int, int]) -> list[tuple[pd.DataFrame, pd.DataFrame, ClassifierMixin, str]]:\n",
    "    results = list[tuple[pd.DataFrame, pd.DataFrame, ClassifierMixin, str]]()\n",
    "    for model in models:\n",
    "        for difficulty in difficulty_indices.keys():\n",
    "            results.append(\n",
    "                (X.iloc[difficulty_indices[difficulty]], y.iloc[difficulty_indices[difficulty]], model, f\"{model.__class__.__name__} \\n baseline performance \\n difficulty level {difficulty}\")\n",
    "            )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestClassifiers(\n",
    "    generate_test_pairs(X_train_resampled, y_train_resampled, models=[rfc, knc, gbc, svc, mlp], difficulty_indices=indices),\n",
    "    n_columns = len(indices.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb141c",
   "metadata": {},
   "source": [
    "# Dodavanje suma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f9181",
   "metadata": {},
   "source": [
    "- Generalizacija: da x% skupa podataka dodamo do y% suma od njegove vrednosti - 100% => random\n",
    "    - Pri tom mozemo da primenimo samo nad atributima, samo nad klasama ili oba => biramo labele koje ce da dobiju noise \n",
    "        - Direktno navodjenjem, N labela kao maksimum ili metoda (sqrt, log nad brojem atributa)\n",
    "        - Da mozemo da biramo kojim ce tezinama da se doda - da vidimo kako utice na pojedine klase\n",
    "    - Pored toga mozemo da ili zamenimo vrednosti ili da ubacimo nove pa da vidimo kako se ponasaju -  da li ce da se prepoznaju i izbace lose instance\n",
    "\n",
    "Znaci:\n",
    "- Funkcija koja uzima:\n",
    "    - X, Y skupovi, difficulty dict\n",
    "    - x% - koliki deo skupa dobija sum\n",
    "    - y% - koliki je procenat suma koji se dodaje ili dict `atribut:procenat suma za taj atribut`\n",
    "    - apply_to - \"all\", \"attributes\", \"classes\"\n",
    "    - labels - tacno koji atributi/klasa dobija sum ili N - broj nasumicnih labela koje dobijaju sum ili metoda\n",
    "    - pre_pick_labels - da li prvo izabere random labels pa doda sum ili svaki put random se dodeljuje\n",
    "    - replace - da li menjamo vrednosti ili nove ubacimo\n",
    "    - difficulty_noise_schedule - `diff: procenat suma` ako drugacije sum primenjujemo nad razlicitim tezinama\n",
    "- Funkcija vraca:\n",
    "    - Staru kopiju podataka\n",
    "    - Novu kopiju podataka\n",
    "    - Indeksi instanci koje su menjane ukoliko stara=nova kopija ILI indeksi novo dodanih instanci\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad6346",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e842b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(X: pd.DataFrame, \n",
    "                   Y: pd.DataFrame, \n",
    "                   difficulty_indices: dict[int, int],\n",
    "                   instance_percentage: float,\n",
    "                   noise_ratio: float,\n",
    "                   attributes: list[str] | int | Callable[[int], int],\n",
    "                   difficulty_noise_schedule: dict[int, float] | None,\n",
    "                   replace: bool = True,\n",
    "                   regenerate_labels: bool = False,\n",
    "                   apply_to: Literal[\"both\",\"attributes\",\"classes\"] = \"attributes\",\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, list[int]]:\n",
    "    \"\"\"\n",
    "        Generates a noisy version of the input dataset\n",
    "        Inputs:\n",
    "            X - the input attributes\n",
    "            Y - the input classes\n",
    "            difficulty_indices - dictionary containing indices for each of the difficulty categories\n",
    "            instance_percentage - percentage of instances in the input (X and Y) that we add noise to\n",
    "            noise_ratio - how much additional noise is added. 0% - no noise added, 100% - completely random value\n",
    "            attributes - which attributes receive noise - list of names, number of attributes or a method (sqrt, log...)\n",
    "            difficulty_noise_schedule - we might want to apply noise to specific percentage of instances with difficulty 0 and another to ones with difficulty 1\n",
    "            replace - do we replace the values or add new instances?\n",
    "            regenerate_labels - do we randomly pick labels every time we apply noise?\n",
    "            apply_to - which columns get noise - attributes, classes or both\n",
    "        Output:\n",
    "            New values of X and Y with added noise\n",
    "            List of indices of newly added or updated instances\n",
    "    \"\"\"\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
